{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10386e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import math\n",
    "import os\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import text\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import huggingface_hub\n",
    "import os\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef7e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Do not duplicate code for this class\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        # TODO Do we have to do max norm here?\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm=1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25bb433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"cbow_final_with_vocab.pt\", map_location=torch.device('cpu'))\n",
    "\n",
    "word2idx = checkpoint['word2idx']\n",
    "idx2word = checkpoint['idx2word']\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 100\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5323f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"sy91dhb\"\n",
    "password = \"g5t49ao\"\n",
    "host = \"178.156.142.230\"\n",
    "port = \"5432\"\n",
    "db = \"hd64m1ki\"\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b749c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT title, url, by, time, score FROM hacker_news.items WHERE type='story' AND dead IS NULL\"\n",
    "df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bbdbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"hackernews_stories.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c261374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"hackernews_stories.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6623154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleWordIndicesDataset(Dataset):\n",
    "    def __init__(self, dataframe, word2idx):\n",
    "        self.samples = []\n",
    "        for _, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=\"Processing titles\"):\n",
    "            title = row['title']\n",
    "            score_val = row['score']\n",
    "            if not isinstance(title, str) or pd.isna(score_val):\n",
    "                continue\n",
    "            score = torch.tensor(score_val, dtype=torch.float32)\n",
    "            words = title.lower().split()\n",
    "            indices = [word2idx[w] for w in words if w in word2idx]\n",
    "            if indices:\n",
    "                self.samples.append((indices, score))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d8329a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5691bbf2d8e14f47abc102d70ab31530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing titles:   0%|          | 0/3413128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e42a96391484a6fac447497762138ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing titles:   0%|          | 0/853282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the dataframe by time\n",
    "df_sorted = df.sort_values('time')\n",
    "\n",
    "# Calculate the split index\n",
    "split_idx = int(len(df_sorted) * 0.8)\n",
    "\n",
    "# Split into train and test\n",
    "df_train = df_sorted.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test = df_sorted.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "# Create datasets\n",
    "title_indices_train = TitleWordIndicesDataset(df_train, word2idx)\n",
    "title_indices_test = TitleWordIndicesDataset(df_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f55425f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleRegressionNN(nn.Module):\n",
    "    def __init__(self, embedding_layer, embedding_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Use the pretrained embedding weights from the CBOW model\n",
    "        self.embeddings = embedding_layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output is a single numeric value\n",
    "\n",
    "    def forward(self, input_indices):\n",
    "        # input_indices: (batch_size, seq_len)\n",
    "        embeds = self.embeddings(input_indices)  # (batch_size, seq_len, embedding_dim)\n",
    "        avg_embeds = embeds.mean(dim=1)         # (batch_size, embedding_dim)\n",
    "        x = self.fc1(avg_embeds)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(-1)  # (batch_size,)\n",
    "\n",
    "model_reg = TitleRegressionNN(model.embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09883bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cd07ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/nn/functional.py:2434: UserWarning: The operator 'aten::embedding_renorm_' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:14.)\n",
      "  torch.embedding_renorm_(weight.detach(), input, max_norm, norm_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/25309], Loss: 3110.7890\n",
      "Epoch [1/3], Step [200/25309], Loss: 3210.9402\n",
      "Epoch [1/3], Step [300/25309], Loss: 3470.4803\n",
      "Epoch [1/3], Step [400/25309], Loss: 3894.3878\n",
      "Epoch [1/3], Step [500/25309], Loss: 2954.6800\n",
      "Epoch [1/3], Step [600/25309], Loss: 4429.9621\n",
      "Epoch [1/3], Step [700/25309], Loss: 4103.9661\n",
      "Epoch [1/3], Step [800/25309], Loss: 3162.6637\n",
      "Epoch [1/3], Step [900/25309], Loss: 4366.0110\n",
      "Epoch [1/3], Step [1000/25309], Loss: 3855.9386\n",
      "Epoch [1/3], Step [1100/25309], Loss: 3193.0165\n",
      "Epoch [1/3], Step [1200/25309], Loss: 3010.4033\n",
      "Epoch [1/3], Step [1300/25309], Loss: 5005.3872\n",
      "Epoch [1/3], Step [1400/25309], Loss: 3720.7254\n",
      "Epoch [1/3], Step [1500/25309], Loss: 3375.5183\n",
      "Epoch [1/3], Step [1600/25309], Loss: 5407.9011\n",
      "Epoch [1/3], Step [1700/25309], Loss: 2861.4455\n",
      "Epoch [1/3], Step [1800/25309], Loss: 3071.5765\n",
      "Epoch [1/3], Step [1900/25309], Loss: 3349.1587\n",
      "Epoch [1/3], Step [2000/25309], Loss: 3192.9925\n",
      "Epoch [1/3], Step [2100/25309], Loss: 4006.9798\n",
      "Epoch [1/3], Step [2200/25309], Loss: 2714.4708\n",
      "Epoch [1/3], Step [2300/25309], Loss: 3482.1212\n",
      "Epoch [1/3], Step [2400/25309], Loss: 3938.4359\n",
      "Epoch [1/3], Step [2500/25309], Loss: 3435.6499\n",
      "Epoch [1/3], Step [2600/25309], Loss: 4267.5117\n",
      "Epoch [1/3], Step [2700/25309], Loss: 3641.3700\n",
      "Epoch [1/3], Step [2800/25309], Loss: 3388.3007\n",
      "Epoch [1/3], Step [2900/25309], Loss: 4569.4571\n",
      "Epoch [1/3], Step [3000/25309], Loss: 3815.9775\n",
      "Epoch [1/3], Step [3100/25309], Loss: 3831.0086\n",
      "Epoch [1/3], Step [3200/25309], Loss: 3083.0692\n",
      "Epoch [1/3], Step [3300/25309], Loss: 3476.7590\n",
      "Epoch [1/3], Step [3400/25309], Loss: 5985.3766\n",
      "Epoch [1/3], Step [3500/25309], Loss: 4165.5379\n",
      "Epoch [1/3], Step [3600/25309], Loss: 5347.6861\n",
      "Epoch [1/3], Step [3700/25309], Loss: 4205.6224\n",
      "Epoch [1/3], Step [3800/25309], Loss: 4014.0184\n",
      "Epoch [1/3], Step [3900/25309], Loss: 3856.6677\n",
      "Epoch [1/3], Step [4000/25309], Loss: 3735.9256\n",
      "Epoch [1/3], Step [4100/25309], Loss: 4224.2824\n",
      "Epoch [1/3], Step [4200/25309], Loss: 3598.3538\n",
      "Epoch [1/3], Step [4300/25309], Loss: 4090.0797\n",
      "Epoch [1/3], Step [4400/25309], Loss: 3157.9545\n",
      "Epoch [1/3], Step [4500/25309], Loss: 3214.4115\n",
      "Epoch [1/3], Step [4600/25309], Loss: 3414.0122\n",
      "Epoch [1/3], Step [4700/25309], Loss: 5645.1094\n",
      "Epoch [1/3], Step [4800/25309], Loss: 3449.2470\n",
      "Epoch [1/3], Step [4900/25309], Loss: 3877.6006\n",
      "Epoch [1/3], Step [5000/25309], Loss: 4257.9727\n",
      "Epoch [1/3], Step [5100/25309], Loss: 3683.2278\n",
      "Epoch [1/3], Step [5200/25309], Loss: 3770.1492\n",
      "Epoch [1/3], Step [5300/25309], Loss: 3480.0629\n",
      "Epoch [1/3], Step [5400/25309], Loss: 5589.1023\n",
      "Epoch [1/3], Step [5500/25309], Loss: 3650.8021\n",
      "Epoch [1/3], Step [5600/25309], Loss: 3311.3773\n",
      "Epoch [1/3], Step [5700/25309], Loss: 3149.7162\n",
      "Epoch [1/3], Step [5800/25309], Loss: 3027.8024\n",
      "Epoch [1/3], Step [5900/25309], Loss: 4827.5438\n",
      "Epoch [1/3], Step [6000/25309], Loss: 3582.1512\n",
      "Epoch [1/3], Step [6100/25309], Loss: 4592.8862\n",
      "Epoch [1/3], Step [6200/25309], Loss: 4014.7811\n",
      "Epoch [1/3], Step [6300/25309], Loss: 3135.2376\n",
      "Epoch [1/3], Step [6400/25309], Loss: 6606.2272\n",
      "Epoch [1/3], Step [6500/25309], Loss: 4148.3544\n",
      "Epoch [1/3], Step [6600/25309], Loss: 3455.2327\n",
      "Epoch [1/3], Step [6700/25309], Loss: 3887.0544\n",
      "Epoch [1/3], Step [6800/25309], Loss: 3179.0644\n",
      "Epoch [1/3], Step [6900/25309], Loss: 4192.3975\n",
      "Epoch [1/3], Step [7000/25309], Loss: 3247.1667\n",
      "Epoch [1/3], Step [7100/25309], Loss: 2765.3626\n",
      "Epoch [1/3], Step [7200/25309], Loss: 3093.0237\n",
      "Epoch [1/3], Step [7300/25309], Loss: 3566.7630\n",
      "Epoch [1/3], Step [7400/25309], Loss: 3490.0559\n",
      "Epoch [1/3], Step [7500/25309], Loss: 3745.4685\n",
      "Epoch [1/3], Step [7600/25309], Loss: 3695.7785\n",
      "Epoch [1/3], Step [7700/25309], Loss: 2997.7004\n",
      "Epoch [1/3], Step [7800/25309], Loss: 3000.0664\n",
      "Epoch [1/3], Step [7900/25309], Loss: 3510.0891\n",
      "Epoch [1/3], Step [8000/25309], Loss: 3489.3297\n",
      "Epoch [1/3], Step [8100/25309], Loss: 3617.2686\n",
      "Epoch [1/3], Step [8200/25309], Loss: 4382.6070\n",
      "Epoch [1/3], Step [8300/25309], Loss: 3369.0657\n",
      "Epoch [1/3], Step [8400/25309], Loss: 3706.1605\n",
      "Epoch [1/3], Step [8500/25309], Loss: 2994.4194\n",
      "Epoch [1/3], Step [8600/25309], Loss: 4059.7503\n",
      "Epoch [1/3], Step [8700/25309], Loss: 3605.2909\n",
      "Epoch [1/3], Step [8800/25309], Loss: 2906.8550\n",
      "Epoch [1/3], Step [8900/25309], Loss: 3829.9407\n",
      "Epoch [1/3], Step [9000/25309], Loss: 3622.7658\n",
      "Epoch [1/3], Step [9100/25309], Loss: 4571.8863\n",
      "Epoch [1/3], Step [9200/25309], Loss: 3405.0133\n",
      "Epoch [1/3], Step [9300/25309], Loss: 3889.6466\n",
      "Epoch [1/3], Step [9400/25309], Loss: 3517.8553\n",
      "Epoch [1/3], Step [9500/25309], Loss: 4016.1576\n",
      "Epoch [1/3], Step [9600/25309], Loss: 3486.0366\n",
      "Epoch [1/3], Step [9700/25309], Loss: 3799.8785\n",
      "Epoch [1/3], Step [9800/25309], Loss: 3847.0326\n",
      "Epoch [1/3], Step [9900/25309], Loss: 3104.8168\n",
      "Epoch [1/3], Step [10000/25309], Loss: 4365.6128\n",
      "Epoch [1/3], Step [10100/25309], Loss: 4393.4571\n",
      "Epoch [1/3], Step [10200/25309], Loss: 3309.7994\n",
      "Epoch [1/3], Step [10300/25309], Loss: 3877.8981\n",
      "Epoch [1/3], Step [10400/25309], Loss: 3492.7736\n",
      "Epoch [1/3], Step [10500/25309], Loss: 4582.4865\n",
      "Epoch [1/3], Step [10600/25309], Loss: 4257.4879\n",
      "Epoch [1/3], Step [10700/25309], Loss: 3034.6403\n",
      "Epoch [1/3], Step [10800/25309], Loss: 3456.9239\n",
      "Epoch [1/3], Step [10900/25309], Loss: 3423.2943\n",
      "Epoch [1/3], Step [11000/25309], Loss: 4767.6469\n",
      "Epoch [1/3], Step [11100/25309], Loss: 4732.8325\n",
      "Epoch [1/3], Step [11200/25309], Loss: 3299.2122\n",
      "Epoch [1/3], Step [11300/25309], Loss: 4967.3238\n",
      "Epoch [1/3], Step [11400/25309], Loss: 5105.0890\n",
      "Epoch [1/3], Step [11500/25309], Loss: 3036.9478\n",
      "Epoch [1/3], Step [11600/25309], Loss: 3479.1485\n",
      "Epoch [1/3], Step [11700/25309], Loss: 3481.3378\n",
      "Epoch [1/3], Step [11800/25309], Loss: 3420.4529\n",
      "Epoch [1/3], Step [11900/25309], Loss: 3711.2604\n",
      "Epoch [1/3], Step [12000/25309], Loss: 4699.8695\n",
      "Epoch [1/3], Step [12100/25309], Loss: 3090.4685\n",
      "Epoch [1/3], Step [12200/25309], Loss: 3405.2314\n",
      "Epoch [1/3], Step [12300/25309], Loss: 3791.4467\n",
      "Epoch [1/3], Step [12400/25309], Loss: 3840.7317\n",
      "Epoch [1/3], Step [12500/25309], Loss: 3561.3496\n",
      "Epoch [1/3], Step [12600/25309], Loss: 3406.0704\n",
      "Epoch [1/3], Step [12700/25309], Loss: 4913.0231\n",
      "Epoch [1/3], Step [12800/25309], Loss: 3352.2128\n",
      "Epoch [1/3], Step [12900/25309], Loss: 3229.6586\n",
      "Epoch [1/3], Step [13000/25309], Loss: 4242.3795\n",
      "Epoch [1/3], Step [13100/25309], Loss: 3788.8500\n",
      "Epoch [1/3], Step [13200/25309], Loss: 3638.3297\n",
      "Epoch [1/3], Step [13300/25309], Loss: 4316.9736\n",
      "Epoch [1/3], Step [13400/25309], Loss: 2681.3354\n",
      "Epoch [1/3], Step [13500/25309], Loss: 3285.6590\n",
      "Epoch [1/3], Step [13600/25309], Loss: 3724.4825\n",
      "Epoch [1/3], Step [13700/25309], Loss: 3890.1247\n",
      "Epoch [1/3], Step [13800/25309], Loss: 2974.8341\n",
      "Epoch [1/3], Step [13900/25309], Loss: 3713.0188\n",
      "Epoch [1/3], Step [14000/25309], Loss: 3447.0117\n",
      "Epoch [1/3], Step [14100/25309], Loss: 3554.8818\n",
      "Epoch [1/3], Step [14200/25309], Loss: 4124.5738\n",
      "Epoch [1/3], Step [14300/25309], Loss: 3817.1928\n",
      "Epoch [1/3], Step [14400/25309], Loss: 4565.0568\n",
      "Epoch [1/3], Step [14500/25309], Loss: 3936.3682\n",
      "Epoch [1/3], Step [14600/25309], Loss: 2770.3650\n",
      "Epoch [1/3], Step [14700/25309], Loss: 4052.0377\n",
      "Epoch [1/3], Step [14800/25309], Loss: 3579.4576\n",
      "Epoch [1/3], Step [14900/25309], Loss: 4225.1094\n",
      "Epoch [1/3], Step [15000/25309], Loss: 4087.8308\n",
      "Epoch [1/3], Step [15100/25309], Loss: 3843.5980\n",
      "Epoch [1/3], Step [15200/25309], Loss: 3779.2804\n",
      "Epoch [1/3], Step [15300/25309], Loss: 3103.2134\n",
      "Epoch [1/3], Step [15400/25309], Loss: 3836.2567\n",
      "Epoch [1/3], Step [15500/25309], Loss: 3012.8826\n",
      "Epoch [1/3], Step [15600/25309], Loss: 3349.9899\n",
      "Epoch [1/3], Step [15700/25309], Loss: 3699.1964\n",
      "Epoch [1/3], Step [15800/25309], Loss: 3443.7786\n",
      "Epoch [1/3], Step [15900/25309], Loss: 3592.9238\n",
      "Epoch [1/3], Step [16000/25309], Loss: 3826.4669\n",
      "Epoch [1/3], Step [16100/25309], Loss: 4030.1482\n",
      "Epoch [1/3], Step [16200/25309], Loss: 3808.7403\n",
      "Epoch [1/3], Step [16300/25309], Loss: 3207.2250\n",
      "Epoch [1/3], Step [16400/25309], Loss: 4066.2834\n",
      "Epoch [1/3], Step [16500/25309], Loss: 4421.1064\n",
      "Epoch [1/3], Step [16600/25309], Loss: 4018.8602\n",
      "Epoch [1/3], Step [16700/25309], Loss: 3429.6345\n",
      "Epoch [1/3], Step [16800/25309], Loss: 3424.9868\n",
      "Epoch [1/3], Step [16900/25309], Loss: 3588.7058\n",
      "Epoch [1/3], Step [17000/25309], Loss: 4158.6044\n",
      "Epoch [1/3], Step [17100/25309], Loss: 2835.2787\n",
      "Epoch [1/3], Step [17200/25309], Loss: 3426.5125\n",
      "Epoch [1/3], Step [17300/25309], Loss: 3035.4118\n",
      "Epoch [1/3], Step [17400/25309], Loss: 4815.4378\n",
      "Epoch [1/3], Step [17500/25309], Loss: 4941.4013\n",
      "Epoch [1/3], Step [17600/25309], Loss: 3291.2501\n",
      "Epoch [1/3], Step [17700/25309], Loss: 3216.7236\n",
      "Epoch [1/3], Step [17800/25309], Loss: 3593.5535\n",
      "Epoch [1/3], Step [17900/25309], Loss: 4390.9604\n",
      "Epoch [1/3], Step [18000/25309], Loss: 3363.1245\n",
      "Epoch [1/3], Step [18100/25309], Loss: 2984.7854\n",
      "Epoch [1/3], Step [18200/25309], Loss: 4415.6209\n",
      "Epoch [1/3], Step [18300/25309], Loss: 4403.2544\n",
      "Epoch [1/3], Step [18400/25309], Loss: 3028.2861\n",
      "Epoch [1/3], Step [18500/25309], Loss: 3484.9519\n",
      "Epoch [1/3], Step [18600/25309], Loss: 3817.2648\n",
      "Epoch [1/3], Step [18700/25309], Loss: 4417.7894\n",
      "Epoch [1/3], Step [18800/25309], Loss: 3189.8606\n",
      "Epoch [1/3], Step [18900/25309], Loss: 4154.2763\n",
      "Epoch [1/3], Step [19000/25309], Loss: 2855.7677\n",
      "Epoch [1/3], Step [19100/25309], Loss: 3512.0142\n",
      "Epoch [1/3], Step [19200/25309], Loss: 3555.2753\n",
      "Epoch [1/3], Step [19300/25309], Loss: 3854.4229\n",
      "Epoch [1/3], Step [19400/25309], Loss: 3241.6034\n",
      "Epoch [1/3], Step [19500/25309], Loss: 4060.6175\n",
      "Epoch [1/3], Step [19600/25309], Loss: 4079.8957\n",
      "Epoch [1/3], Step [19700/25309], Loss: 3750.7023\n",
      "Epoch [1/3], Step [19800/25309], Loss: 3611.5137\n",
      "Epoch [1/3], Step [19900/25309], Loss: 4097.9963\n",
      "Epoch [1/3], Step [20000/25309], Loss: 3728.8688\n",
      "Epoch [1/3], Step [20100/25309], Loss: 2762.6048\n",
      "Epoch [1/3], Step [20200/25309], Loss: 3317.3324\n",
      "Epoch [1/3], Step [20300/25309], Loss: 4577.7042\n",
      "Epoch [1/3], Step [20400/25309], Loss: 3972.5367\n",
      "Epoch [1/3], Step [20500/25309], Loss: 3019.8772\n",
      "Epoch [1/3], Step [20600/25309], Loss: 3667.0514\n",
      "Epoch [1/3], Step [20700/25309], Loss: 3890.1215\n",
      "Epoch [1/3], Step [20800/25309], Loss: 3515.6622\n",
      "Epoch [1/3], Step [20900/25309], Loss: 4600.0558\n",
      "Epoch [1/3], Step [21000/25309], Loss: 4193.6288\n",
      "Epoch [1/3], Step [21100/25309], Loss: 3126.3530\n",
      "Epoch [1/3], Step [21200/25309], Loss: 3618.5222\n",
      "Epoch [1/3], Step [21300/25309], Loss: 4073.7988\n",
      "Epoch [1/3], Step [21400/25309], Loss: 4542.4478\n",
      "Epoch [1/3], Step [21500/25309], Loss: 3468.8965\n",
      "Epoch [1/3], Step [21600/25309], Loss: 3357.6199\n",
      "Epoch [1/3], Step [21700/25309], Loss: 4068.3265\n",
      "Epoch [1/3], Step [21800/25309], Loss: 3555.8750\n",
      "Epoch [1/3], Step [21900/25309], Loss: 4217.8288\n",
      "Epoch [1/3], Step [22000/25309], Loss: 3312.5428\n",
      "Epoch [1/3], Step [22100/25309], Loss: 3189.0274\n",
      "Epoch [1/3], Step [22200/25309], Loss: 4172.0695\n",
      "Epoch [1/3], Step [22300/25309], Loss: 4002.7807\n",
      "Epoch [1/3], Step [22400/25309], Loss: 3473.5085\n",
      "Epoch [1/3], Step [22500/25309], Loss: 3714.8819\n",
      "Epoch [1/3], Step [22600/25309], Loss: 3455.9245\n",
      "Epoch [1/3], Step [22700/25309], Loss: 3642.9766\n",
      "Epoch [1/3], Step [22800/25309], Loss: 4168.7236\n",
      "Epoch [1/3], Step [22900/25309], Loss: 4210.2925\n",
      "Epoch [1/3], Step [23000/25309], Loss: 4334.3099\n",
      "Epoch [1/3], Step [23100/25309], Loss: 3224.4480\n",
      "Epoch [1/3], Step [23200/25309], Loss: 4000.0589\n",
      "Epoch [1/3], Step [23300/25309], Loss: 3396.9213\n",
      "Epoch [1/3], Step [23400/25309], Loss: 2937.8046\n",
      "Epoch [1/3], Step [23500/25309], Loss: 2862.2884\n",
      "Epoch [1/3], Step [23600/25309], Loss: 3406.1913\n",
      "Epoch [1/3], Step [23700/25309], Loss: 3267.7577\n",
      "Epoch [1/3], Step [23800/25309], Loss: 3863.5881\n",
      "Epoch [1/3], Step [23900/25309], Loss: 4054.0028\n",
      "Epoch [1/3], Step [24000/25309], Loss: 4038.3163\n",
      "Epoch [1/3], Step [24100/25309], Loss: 2778.7757\n",
      "Epoch [1/3], Step [24200/25309], Loss: 4164.6173\n",
      "Epoch [1/3], Step [24300/25309], Loss: 3109.8360\n",
      "Epoch [1/3], Step [24400/25309], Loss: 3493.9403\n",
      "Epoch [1/3], Step [24500/25309], Loss: 4406.4673\n",
      "Epoch [1/3], Step [24600/25309], Loss: 4263.9996\n",
      "Epoch [1/3], Step [24700/25309], Loss: 4089.5298\n",
      "Epoch [1/3], Step [24800/25309], Loss: 2923.2650\n",
      "Epoch [1/3], Step [24900/25309], Loss: 3358.4770\n",
      "Epoch [1/3], Step [25000/25309], Loss: 3910.2776\n",
      "Epoch [1/3], Step [25100/25309], Loss: 3215.3642\n",
      "Epoch [1/3], Step [25200/25309], Loss: 4000.3963\n",
      "Epoch [1/3], Step [25300/25309], Loss: 3563.3151\n",
      "Epoch [2/3], Step [100/25309], Loss: 3141.9331\n",
      "Epoch [2/3], Step [200/25309], Loss: 3261.4800\n",
      "Epoch [2/3], Step [300/25309], Loss: 3344.3646\n",
      "Epoch [2/3], Step [400/25309], Loss: 4056.0998\n",
      "Epoch [2/3], Step [500/25309], Loss: 2959.9426\n",
      "Epoch [2/3], Step [600/25309], Loss: 3758.2666\n",
      "Epoch [2/3], Step [700/25309], Loss: 2963.7298\n",
      "Epoch [2/3], Step [800/25309], Loss: 4050.8435\n",
      "Epoch [2/3], Step [900/25309], Loss: 3576.7538\n",
      "Epoch [2/3], Step [1000/25309], Loss: 3280.0888\n",
      "Epoch [2/3], Step [1100/25309], Loss: 3377.9041\n",
      "Epoch [2/3], Step [1200/25309], Loss: 4038.7455\n",
      "Epoch [2/3], Step [1300/25309], Loss: 3730.3549\n",
      "Epoch [2/3], Step [1400/25309], Loss: 3569.8832\n",
      "Epoch [2/3], Step [1500/25309], Loss: 3657.3280\n",
      "Epoch [2/3], Step [1600/25309], Loss: 3267.3095\n",
      "Epoch [2/3], Step [1700/25309], Loss: 6751.0513\n",
      "Epoch [2/3], Step [1800/25309], Loss: 3421.0635\n",
      "Epoch [2/3], Step [1900/25309], Loss: 3800.8229\n",
      "Epoch [2/3], Step [2000/25309], Loss: 4003.5231\n",
      "Epoch [2/3], Step [2100/25309], Loss: 3332.1410\n",
      "Epoch [2/3], Step [2200/25309], Loss: 4687.3523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_scores)\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 39\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.11/site-packages/torch/optim/adam.py:476\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    474\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    478\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 128\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Move model to device\n",
    "model_reg = model_reg.to(device)\n",
    "\n",
    "# Prepare DataLoader with padding for variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of (indices, score) tuples\n",
    "    indices_list, scores_list = zip(*batch)\n",
    "    lengths = [len(x) for x in indices_list]\n",
    "    max_len = max(lengths)\n",
    "    padded = [x + [0]*(max_len - len(x)) for x in indices_list]\n",
    "    padded_indices = torch.tensor(padded, dtype=torch.long)\n",
    "    scores = torch.tensor(scores_list, dtype=torch.float32)\n",
    "    return padded_indices, scores\n",
    "\n",
    "train_loader = DataLoader(title_indices_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Prepare target scores\n",
    "train_scores = torch.tensor(df_train['score'].values, dtype=torch.float32)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_reg.parameters(), lr=learning_rate)\n",
    "\n",
    "model_reg.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (batch_indices, batch_scores) in enumerate(train_loader):\n",
    "        batch_indices, batch_scores = batch_indices.to(device), batch_scores.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_reg(batch_indices)\n",
    "        loss = criterion(outputs, batch_scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            avg_loss = running_loss / 100\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdad4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackerNewsDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Example: return title, score as tensor, and author\n",
    "        # You can customize this as needed\n",
    "        title = row['title']\n",
    "        score = torch.tensor(row['score'], dtype=torch.float32)\n",
    "        author = row['by']\n",
    "        return {\"title\": title, \"score\": score, \"author\": author}\n",
    "\n",
    "# Example usage:\n",
    "hn_dataset = HackerNewsDataset(df)\n",
    "hn_loader = DataLoader(hn_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc8dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by time\n",
    "df_sorted = df.sort_values('time')\n",
    "\n",
    "# Calculate the split index\n",
    "split_idx = int(len(df_sorted) * 0.8)\n",
    "\n",
    "# Split into train and test\n",
    "df_train = df_sorted.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test = df_sorted.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "# Create datasets\n",
    "hn_train = HackerNewsDataset(df_train)\n",
    "hn_test = HackerNewsDataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109be79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
