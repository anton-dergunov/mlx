{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d89f797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.venv/lib/python3.11/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.1. Bad things might happen unless you revert torch to 1.x.\n",
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: ru (0.85) in first 30s of audio...\n",
      "[{'text': ' –í–æ —Å–∏—Ö –≤–∞–Ω, —Å–≤–∞—Ç–∏!', 'start': 0.031, 'end': 1.499}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33msegments\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;66;03m# before alignment\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# delete model if low on GPU resources\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 2. Align whisper output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msegments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m = \u001b[33m\"\u001b[39m\u001b[33mÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m model_a, metadata = whisperx.load_align_model(language_code=result[\u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m], device=device)\n\u001b[32m     26\u001b[39m result = whisperx.align(result[\u001b[33m\"\u001b[39m\u001b[33msegments\u001b[39m\u001b[33m\"\u001b[39m], model_a, metadata, audio, device, return_char_alignments=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "\n",
    "device = \"cpu\"\n",
    "audio_file = \"/Users/anton/Downloads/audio_trim.wav\"\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"int8\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "\n",
    "# save model to local path (optional)\n",
    "# model_dir = \"/path/\"\n",
    "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
    "\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "# 2. Align whisper output\n",
    "result[\"segments\"][\"text\"] = \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "print(result[\"segments\"]) # after alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; import torch; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "diarize_model = whisperx.diarize.DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)\n",
    "\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "870a5267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segments': [{'text': ' –í–æ —Å–∏—Ö –≤–∞–Ω, —Å–≤–∞—Ç–∏!', 'start': 0.031, 'end': 1.499}],\n",
       " 'language': 'ru'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867eff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0.031, 'end': 1.52, 'text': 'ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†', 'words': [{'word': 'Êàë', 'start': np.float64(0.031), 'end': np.float64(1.394), 'score': np.float64(0.908)}, {'word': 'Âñú', 'start': np.float64(1.394), 'end': np.float64(1.415), 'score': np.float64(0.0)}, {'word': 'Ê¨¢', 'start': np.float64(1.415), 'end': np.float64(1.436), 'score': np.float64(0.0)}, {'word': 'Êú∫', 'start': np.float64(1.436), 'end': np.float64(1.457), 'score': np.float64(0.0)}, {'word': 'Âô®', 'start': np.float64(1.457), 'end': np.float64(1.478), 'score': np.float64(0.0)}, {'word': 'Â≠¶', 'start': np.float64(1.478), 'end': np.float64(1.499), 'score': np.float64(0.0)}, {'word': '‰π†', 'start': np.float64(1.499), 'end': np.float64(1.52), 'score': np.float64(0.0)}]}]\n"
     ]
    }
   ],
   "source": [
    "result[\"segments\"][0][\"text\"] = \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"zh\", device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "print(result[\"segments\"]) # after alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9864fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: ÊàëÂñúÊ¨æÈöèËµ∑\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_audio_backend' from 'torchaudio.backend.common' (/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/torchaudio/backend/common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m log_probs = log_probs[\u001b[32m0\u001b[39m].cpu().detach().numpy()\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# üëá Generate alignment with torchaudio CTC forced alignment utilities\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_audio_backend\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlog_probs shape:\u001b[39m\u001b[33m\"\u001b[39m, log_probs.shape)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFirst log_probs frame:\u001b[39m\u001b[33m\"\u001b[39m, log_probs[\u001b[32m0\u001b[39m][:\u001b[32m5\u001b[39m])\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_audio_backend' from 'torchaudio.backend.common' (/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/torchaudio/backend/common.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.models import wav2vec2_model\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from torchaudio.transforms import Resample\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# üìå Example: Mandarin Wav2Vec2 CTC model (use a multilingual model if no Chinese-specific)\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# üß© Load sample audio (replace with your own file)\n",
    "# Example: Load with librosa\n",
    "wav_path = \"/Users/anton/Downloads/audio_trim.wav\"\n",
    "waveform, sr = librosa.load(wav_path, sr=16000)\n",
    "input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "# üß© Inference: Get logits\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# üß© Get predicted tokens\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "print(\"Transcript:\", transcription)\n",
    "\n",
    "# üß© Forced alignment: use CTC segmentation\n",
    "from torchaudio.models import RNNT\n",
    "\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "\n",
    "# Torchaudio has CTC forced alignment utils\n",
    "import torchaudio.functional as F\n",
    "\n",
    "# üëá Extract log_probs for alignment\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "log_probs = log_probs[0].cpu().detach().numpy()\n",
    "\n",
    "# üëá Generate alignment with torchaudio CTC forced alignment utilities\n",
    "from torchaudio.backend.common import get_audio_backend\n",
    "\n",
    "print(\"log_probs shape:\", log_probs.shape)\n",
    "print(\"First log_probs frame:\", log_probs[0][:5])\n",
    "\n",
    "# For full forced alignment, use torchaudio's `ctc_segmentation` helper:\n",
    "from torchaudio.models.ctc_segmentation import ctc_segmentation, CtcSegmentationParameters, prepare_token_list\n",
    "\n",
    "# Tokenize target text: same vocab as processor\n",
    "target_text = \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"\n",
    "tokens = processor.tokenizer.convert_tokens_to_ids(processor.tokenizer.tokenize(target_text))\n",
    "print(\"Target text tokens:\", tokens)\n",
    "\n",
    "# CTC parameters\n",
    "params = CtcSegmentationParameters()\n",
    "params.char_list = processor.tokenizer.convert_ids_to_tokens(range(len(processor.tokenizer)))\n",
    "params.blank = processor.tokenizer.pad_token_id\n",
    "\n",
    "# Compute segmentation\n",
    "# 1) Prepare ground truth\n",
    "ground_truth_mat, utt_begin_indices = prepare_token_list(params, [target_text])\n",
    "\n",
    "# 2) Run segmentation\n",
    "timings, char_probs, state_list = ctc_segmentation(\n",
    "    params,\n",
    "    log_probs,\n",
    "    ground_truth_mat\n",
    ")\n",
    "\n",
    "print(\"Alignment timings:\", timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: ÊàëÂñúÊ¨æÈöèËµ∑\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchaudio.models.ctc_segmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTranscript:\u001b[39m\u001b[33m\"\u001b[39m, transcription)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# üß© Forced alignment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mctc_segmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     ctc_segmentation,\n\u001b[32m     29\u001b[39m     CtcSegmentationParameters,\n\u001b[32m     30\u001b[39m     prepare_token_list,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# üìå Prepare target text\u001b[39;00m\n\u001b[32m     34\u001b[39m target_text = \u001b[33m\"\u001b[39m\u001b[33mÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchaudio.models.ctc_segmentation'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import librosa\n",
    "\n",
    "# üìå Load Wav2Vec2 Chinese model\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# üß© Load audio\n",
    "wav_path = \"/Users/anton/Downloads/audio_trim.wav\"\n",
    "waveform, sr = librosa.load(wav_path, sr=16000)\n",
    "input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "# üß© Run CTC\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# üìå Decode text\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "print(\"Transcript:\", transcription)\n",
    "\n",
    "# üß© Forced alignment\n",
    "from torchaudio.models.ctc_segmentation import (\n",
    "    ctc_segmentation,\n",
    "    CtcSegmentationParameters,\n",
    "    prepare_token_list,\n",
    ")\n",
    "\n",
    "# üìå Prepare target text\n",
    "target_text = \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"\n",
    "tokens = processor.tokenizer.convert_tokens_to_ids(\n",
    "    processor.tokenizer.tokenize(target_text)\n",
    ")\n",
    "print(\"Target tokens:\", tokens)\n",
    "\n",
    "# üìå Log probs\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "log_probs = log_probs[0].cpu().numpy()\n",
    "\n",
    "# üìå CTC params\n",
    "params = CtcSegmentationParameters()\n",
    "params.char_list = processor.tokenizer.convert_ids_to_tokens(\n",
    "    range(len(processor.tokenizer))\n",
    ")\n",
    "params.blank = processor.tokenizer.pad_token_id\n",
    "\n",
    "# üìå Prepare alignment input\n",
    "ground_truth_mat, utt_begin_indices = prepare_token_list(params, [target_text])\n",
    "\n",
    "# üìå Do segmentation\n",
    "timings, char_probs, state_list = ctc_segmentation(\n",
    "    params,\n",
    "    log_probs,\n",
    "    ground_truth_mat\n",
    ")\n",
    "\n",
    "print(\"CTC timings:\\n\", timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3b2defa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rj/d19lxfn1483btqgkk1kgg95c0000gp/T/ipykernel_94481/3684232437.py\", line 6, in <module>\n",
      "    from ctc_segmentation import ctc_segmentation, CtcSegmentationParameters, prepare_text\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ctc_segmentation/__init__.py\", line 2, in <module>\n",
      "    from .ctc_segmentation import ctc_segmentation\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ctc_segmentation/ctc_segmentation.py\", line 27, in <module>\n",
      "    from .ctc_segmentation_dyn import cython_fill_table\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/rj/d19lxfn1483btqgkk1kgg95c0000gp/T/ipykernel_94481/3684232437.py\", line 6, in <module>\n",
      "    from ctc_segmentation import ctc_segmentation, CtcSegmentationParameters, prepare_text\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ctc_segmentation/__init__.py\", line 2, in <module>\n",
      "    from .ctc_segmentation import ctc_segmentation\n",
      "  File \"/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/ctc_segmentation/ctc_segmentation.py\", line 32, in <module>\n",
      "    from .ctc_segmentation_dyn import cython_fill_table\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlx/src/week5/.venv/lib/python3.11/site-packages/ctc_segmentation/ctc_segmentation.py:27\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mctc_segmentation_dyn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cython_fill_table\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mctc_segmentation/ctc_segmentation_dyn.pyx:1\u001b[39m, in \u001b[36minit ctc_segmentation.ctc_segmentation_dyn\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2Processor, Wav2Vec2ForCTC\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlibrosa\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctc_segmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ctc_segmentation, CtcSegmentationParameters, prepare_text\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m      9\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mjonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlx/src/week5/.venv/lib/python3.11/site-packages/ctc_segmentation/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Import all functions of the CTC segmentation package.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mctc_segmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ctc_segmentation\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mctc_segmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CtcSegmentationParameters\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mctc_segmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m determine_utterance_segments\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlx/src/week5/.venv/lib/python3.11/site-packages/ctc_segmentation/ctc_segmentation.py:32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyximport\u001b[39;00m\n\u001b[32m     31\u001b[39m     pyximport.install(setup_args={\u001b[33m\"\u001b[39m\u001b[33minclude_dirs\u001b[39m\u001b[33m\"\u001b[39m: np.get_include()})\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mctc_segmentation_dyn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cython_fill_table\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCtcSegmentationParameters\u001b[39;00m:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Default values for CTC segmentation.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    May need adjustment according to localization or ASR settings.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m \u001b[33;03m    these settings.\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mctc_segmentation/ctc_segmentation_dyn.pyx:1\u001b[39m, in \u001b[36minit ctc_segmentation.ctc_segmentation_dyn\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import librosa\n",
    "\n",
    "from ctc_segmentation import ctc_segmentation, CtcSegmentationParameters, prepare_text\n",
    "\n",
    "# Load model\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Load audio\n",
    "wav_path = \"/Users/anton/Downloads/audio_trim.wav\"\n",
    "waveform, sr = librosa.load(wav_path, sr=16000)\n",
    "input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "# Get logits\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# Decode\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "log_probs = log_probs[0].cpu().numpy()\n",
    "\n",
    "# Target text\n",
    "ground_truth = [\"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"]\n",
    "\n",
    "# CTC seg params\n",
    "params = CtcSegmentationParameters()\n",
    "char_list = list(processor.tokenizer.get_vocab().keys())\n",
    "\n",
    "ground_truth_mat, utt_begin_indices = prepare_text(params, char_list, ground_truth)\n",
    "\n",
    "timings, char_probs, state_list = ctc_segmentation(params, log_probs, ground_truth_mat)\n",
    "\n",
    "print(\"Timings:\", timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1a124",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute '__version___'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version___\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlx/src/week5/.venv/lib/python3.11/site-packages/numpy/__init__.py:414\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    411\u001b[39m import os\n\u001b[32m    412\u001b[39m use_hugepage = os.environ.get(\"NUMPY_MADVISE_HUGEPAGE\", None)\n\u001b[32m    413\u001b[39m if sys.platform == \"linux\" and use_hugepage is None:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     # If there is an issue with parsing the kernel version,\n\u001b[32m    415\u001b[39m     # set use_hugepages to 0. Usage of LooseVersion will handle\n\u001b[32m    416\u001b[39m     # the kernel version parsing better, but avoided since it\n\u001b[32m    417\u001b[39m     # will increase the import time. See: #16679 for related discussion.\n\u001b[32m    418\u001b[39m     try:\n\u001b[32m    419\u001b[39m         use_hugepage = 1\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute '__version___'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f921a578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper transcript: ÊàëÂñúÊ¨æÈöèËµ∑\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchaudio.models.ctc_segmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWhisper transcript:\u001b[39m\u001b[33m\"\u001b[39m, whispered)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 5Ô∏è‚É£ Prepare for forced-alignment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mctc_segmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     CtcSegmentationParameters,\n\u001b[32m     27\u001b[39m     ctc_segmentation,\n\u001b[32m     28\u001b[39m     prepare_token_list,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m target_text = \u001b[33m\"\u001b[39m\u001b[33mÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Token sequences accepted by CTC:\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchaudio.models.ctc_segmentation'"
     ]
    }
   ],
   "source": [
    "import torch, librosa, numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torchaudio\n",
    "\n",
    "# 1Ô∏è‚É£ Load pre‚Äëtrained Wav2Vec2 model & processor\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).eval()\n",
    "\n",
    "# 2Ô∏è‚É£ Load and preprocess audio\n",
    "waveform, sr = librosa.load(\"/Users/anton/Downloads/audio_trim.wav\", sr=16000)\n",
    "input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "# 3Ô∏è‚É£ Compute logits and take softmax\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "# 4Ô∏è‚É£ Generate whispered transcript (for reference)\n",
    "pred_ids = logits.argmax(dim=-1)[0]\n",
    "whispered = processor.decode(pred_ids)\n",
    "print(\"Whisper transcript:\", whispered)\n",
    "\n",
    "# 5Ô∏è‚É£ Prepare for forced-alignment\n",
    "from torchaudio.models.ctc_segmentation import (\n",
    "    CtcSegmentationParameters,\n",
    "    ctc_segmentation,\n",
    "    prepare_token_list,\n",
    ")\n",
    "\n",
    "target_text = \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"\n",
    "# Token sequences accepted by CTC:\n",
    "ground_truth_mat, utt_begin_indices = prepare_token_list(\n",
    "    CtcSegmentationParameters(), [target_text]\n",
    ")\n",
    "# Character list for mapping:\n",
    "params = CtcSegmentationParameters()\n",
    "params.char_list = list(processor.tokenizer.get_vocab().keys())\n",
    "params.blank = processor.tokenizer.pad_token_id\n",
    "\n",
    "# 6Ô∏è‚É£ Run forced-alignment\n",
    "timings, _, _ = ctc_segmentation(params, log_probs, ground_truth_mat)\n",
    "\n",
    "# üìå Convert frame indices to seconds\n",
    "frame_duration = model.config.conv_stride[-1] / sr\n",
    "word_times = [(start*frame_duration, end*frame_duration, target_text[i])\n",
    "              for i, (start, end, _) in enumerate(timings)]\n",
    "\n",
    "print(\"Word timings:\", word_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dac45db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/Users/anton/mlx/src/week5/.venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Wav2Vec2CTCTokenizer(name_or_path='jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn', vocab_size=3503, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"<pad>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n\t1: AddedToken(\"<s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n\t2: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n\t3: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n}\n) got multiple values for keyword argument 'return_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    116\u001b[39m words = \u001b[38;5;28mlist\u001b[39m(transcription)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m processor.as_target_processor():\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     labels = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.input_ids\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# 5. Get Model Emissions\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlx/src/week5/.venv/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:102\u001b[39m, in \u001b[36mWav2Vec2Processor.__call__\u001b[39m\u001b[34m(self, audio, text, images, videos, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_processor(\n\u001b[32m    103\u001b[39m         audio,\n\u001b[32m    104\u001b[39m         **output_kwargs[\u001b[33m\"\u001b[39m\u001b[33maudio_kwargs\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    105\u001b[39m         **output_kwargs[\u001b[33m\"\u001b[39m\u001b[33mtext_kwargs\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    106\u001b[39m         **output_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcommon_kwargs\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    107\u001b[39m     )\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    110\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.feature_extractor(audio, **output_kwargs[\u001b[33m\"\u001b[39m\u001b[33maudio_kwargs\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: Wav2Vec2CTCTokenizer(name_or_path='jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn', vocab_size=3503, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"<pad>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n\t1: AddedToken(\"<s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n\t2: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n\t3: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n}\n) got multiple values for keyword argument 'return_tensors'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# --- Core Alignment Functions (Simplified from whisperx) ---\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    score: float\n",
    "\n",
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    score: float\n",
    "\n",
    "def get_trellis(emission, tokens, blank_id=0):\n",
    "    \"\"\"Calculates the alignment trellis for CTC.\"\"\"\n",
    "    num_frame = emission.size(0)\n",
    "    num_tokens = len(tokens)\n",
    "    trellis = torch.full((num_frame, num_tokens), -float(\"inf\"))\n",
    "\n",
    "    trellis[0, 0] = emission[0, blank_id]\n",
    "    trellis[0, 1] = emission[0, tokens[1]]\n",
    "\n",
    "    for t in range(1, num_frame):\n",
    "        trellis[t, 0] = trellis[t - 1, 0] + emission[t, blank_id]\n",
    "        for j in range(1, num_tokens):\n",
    "            trellis[t, j] = max(trellis[t - 1, j], trellis[t - 1, j - 1]) + emission[t, tokens[j]]\n",
    "            if j > 1 and tokens[j-1] == tokens[j]:\n",
    "                 trellis[t,j] = max(trellis[t,j], trellis[t-1,j-2] + emission[t,tokens[j]])\n",
    "    return trellis\n",
    "\n",
    "\n",
    "def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "    \"\"\"Backtracks through the trellis to find the most likely alignment path.\"\"\"\n",
    "    j = trellis.size(1) - 1\n",
    "    t = trellis.size(0) - 1\n",
    "    path = [Point(j, t, emission[t, tokens[j]].exp().item())]\n",
    "\n",
    "    while j > 0 and t > 0:\n",
    "        # 1. Look for a diagonal move (token change)\n",
    "        if trellis[t-1, j-1] > trellis[t-1,j] and tokens[j-1] != tokens[j]:\n",
    "            j = j-1\n",
    "        \n",
    "        # 2. Otherwise, stay at the same token (horizontal move)\n",
    "        t = t - 1\n",
    "        \n",
    "        # Use the token from the path's last point if it's a non-blank token, else use the new 'j'\n",
    "        current_token_idx = path[-1].token_index\n",
    "        token_id = tokens[current_token_idx] if tokens[current_token_idx] != blank_id else tokens[j]\n",
    "        prob = emission[t, token_id].exp().item()\n",
    "        path.append(Point(j, t, prob))\n",
    "\n",
    "\n",
    "    while t > 0:\n",
    "        t -= 1\n",
    "        prob = emission[t, blank_id].exp().item()\n",
    "        path.append(Point(0, t, prob))\n",
    "        \n",
    "    return path[::-1]\n",
    "\n",
    "\n",
    "def merge_repeats(path, transcript):\n",
    "    \"\"\"Merges consecutive repeated tokens in the alignment path.\"\"\"\n",
    "    i1, i2 = 0, 0\n",
    "    segments = []\n",
    "    while i1 < len(path):\n",
    "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "            i2 += 1\n",
    "        \n",
    "        # Correctly get the character for the segment\n",
    "        char_idx = path[i1].token_index\n",
    "        # In the modified trellis, tokens are padded, so we need to adjust the index\n",
    "        label = transcript[(char_idx - 1) // 2]\n",
    "        \n",
    "        score = sum(p.score for p in path[i1:i2]) / (i2 - i1)\n",
    "        segments.append(\n",
    "            Segment(\n",
    "                label,\n",
    "                path[i1].time_index,\n",
    "                path[i2 - 1].time_index + 1,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "        i1 = i2\n",
    "    return segments\n",
    "\n",
    "# --- Main Alignment Logic ---\n",
    "\n",
    "# 1. Setup: Define file paths, transcription, and model details\n",
    "audio_path = \"/Users/anton/Downloads/audio_trim.wav\"\n",
    "transcription = \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 2. Load Model and Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 3. Load and Preprocess Audio\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "audio = waveform.squeeze().to(device)\n",
    "\n",
    "# 4. Prepare Transcription Tokens\n",
    "# The characters are the \"words\" for Chinese\n",
    "words = list(transcription)\n",
    "with processor.as_target_processor():\n",
    "    labels = processor(transcription, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 5. Get Model Emissions\n",
    "with torch.no_grad():\n",
    "    logits = model(audio.unsqueeze(0)).logits\n",
    "emissions = torch.log_softmax(logits, dim=-1)[0].cpu()\n",
    "\n",
    "# 6. Generate Alignment\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "# Insert blanks between characters for CTC alignment\n",
    "tokens = []\n",
    "for l in labels[0]:\n",
    "    tokens.append(blank_id)\n",
    "    tokens.append(l.item())\n",
    "tokens.append(blank_id)\n",
    "\n",
    "\n",
    "trellis = get_trellis(emissions, tokens, blank_id)\n",
    "path = backtrack(trellis, emissions, tokens, blank_id)\n",
    "\n",
    "# Filter out blank tokens from the path before merging repeats\n",
    "# Path indices need to correspond to the non-blank tokens in `transcript`\n",
    "filtered_path = [p for p in path if p.token_index % 2 != 0]\n",
    "\n",
    "segments = merge_repeats(filtered_path, transcription)\n",
    "\n",
    "\n",
    "# 7. Format Output\n",
    "word_segments = []\n",
    "# Ratio to convert frame indices to seconds\n",
    "ratio = audio.shape[0] / emissions.shape[0] / 16000\n",
    "\n",
    "for seg in segments:\n",
    "    word_segments.append(\n",
    "        {\n",
    "            \"word\": seg.label,\n",
    "            \"start\": round(seg.start * ratio, 3),\n",
    "            \"end\": round(seg.end * ratio, 3),\n",
    "            \"score\": round(seg.score, 3),\n",
    "        }\n",
    "    )\n",
    "\n",
    "final_result = [{\n",
    "    \"start\": word_segments[0][\"start\"],\n",
    "    \"end\": word_segments[-1][\"end\"],\n",
    "    \"text\": transcription,\n",
    "    \"words\": word_segments,\n",
    "}]\n",
    "\n",
    "# 8. Print the final result\n",
    "import json\n",
    "print(json.dumps(final_result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a95ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"start\": 0.06,\n",
      "    \"end\": 1.27,\n",
      "    \"text\": \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\",\n",
      "    \"words\": [\n",
      "      {\n",
      "        \"word\": \"Êàë\",\n",
      "        \"start\": 0.06,\n",
      "        \"end\": 0.081,\n",
      "        \"score\": 0.993\n",
      "      },\n",
      "      {\n",
      "        \"word\": \"Âñú\",\n",
      "        \"start\": 0.423,\n",
      "        \"end\": 0.444,\n",
      "        \"score\": 0.99\n",
      "      },\n",
      "      {\n",
      "        \"word\": \"Ê¨¢\",\n",
      "        \"start\": 0.605,\n",
      "        \"end\": 0.625,\n",
      "        \"score\": 0.004\n",
      "      },\n",
      "      {\n",
      "        \"word\": \"Êú∫\",\n",
      "        \"start\": 0.847,\n",
      "        \"end\": 0.867,\n",
      "        \"score\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"word\": \"Âô®\",\n",
      "        \"start\": 0.948,\n",
      "        \"end\": 0.968,\n",
      "        \"score\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"word\": \"Â≠¶\",\n",
      "        \"start\": 1.109,\n",
      "        \"end\": 1.129,\n",
      "        \"score\": 0.0\n",
      "      },\n",
      "      {\n",
      "        \"word\": \"‰π†\",\n",
      "        \"start\": 1.25,\n",
      "        \"end\": 1.27,\n",
      "        \"score\": 0.0\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "\n",
    "def align_audio_to_text(audio_path, transcription):\n",
    "    \"\"\"\n",
    "    Performs forced alignment of an audio file to its transcription using a Wav2Vec2 model.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file (e.g., .wav).\n",
    "        transcription (str): The text transcription corresponding to the audio.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing a dictionary with the full segment and word-level (character-level for Chinese) timestamps.\n",
    "    \"\"\"\n",
    "    # --- 1. Setup Model and Audio ---\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "    \n",
    "    # Load model and processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Load and resample audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # --- 2. Get Model Predictions ---\n",
    "    # Process audio and text\n",
    "    input_values = processor(waveform.squeeze(), return_tensors=\"pt\", sampling_rate=16000).input_values.to(device)\n",
    "    labels = processor(text=transcription, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Get emission probabilities\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    emissions = torch.log_softmax(logits, dim=-1)[0].cpu()\n",
    "    \n",
    "    # --- 3. Core Alignment Algorithm (CTC Forced Alignment) ---\n",
    "    # Create the trellis (dynamic programming table)\n",
    "    blank_id = processor.tokenizer.pad_token_id\n",
    "    tokens = labels[0].tolist()\n",
    "    \n",
    "    # Insert blank tokens between characters for CTC\n",
    "    token_path = [blank_id] + [val for t in tokens for val in (t, blank_id)]\n",
    "    \n",
    "    trellis = torch.full((emissions.shape[0], len(token_path)), -float(\"inf\"))\n",
    "    trellis[0, 0] = emissions[0, blank_id]\n",
    "    trellis[0, 1] = emissions[0, token_path[1]]\n",
    "\n",
    "    for t in range(1, emissions.shape[0]):\n",
    "        for j in range(len(token_path)):\n",
    "            # Case 1: Stay at the same token (can be blank or a character)\n",
    "            prev_trellis = trellis[t - 1, j]\n",
    "            # Case 2: Move from the previous token\n",
    "            if j > 0:\n",
    "                prev_trellis = max(prev_trellis, trellis[t-1, j-1])\n",
    "            # Case 3: Skip a blank token\n",
    "            if j > 1 and token_path[j] != blank_id and token_path[j-2] == token_path[j]:\n",
    "                prev_trellis = max(prev_trellis, trellis[t-1, j-2])\n",
    "            \n",
    "            trellis[t, j] = prev_trellis + emissions[t, token_path[j]]\n",
    "\n",
    "    # Backtrack to find the most likely path\n",
    "    path = []\n",
    "    j = trellis.shape[1] - 1\n",
    "    for t in range(trellis.shape[0] - 1, -1, -1):\n",
    "        # Find the index of the maximum predecessor in the trellis\n",
    "        if j > 1 and token_path[j] != blank_id and token_path[j-2] == token_path[j] and trellis[t-1,j-2] >= trellis[t-1,j-1] and trellis[t-1,j-2] >= trellis[t-1,j]:\n",
    "             path.append((token_path[j], t, emissions[t, token_path[j]].exp().item()))\n",
    "             j = j-2\n",
    "        elif j > 0 and trellis[t-1, j-1] >= trellis[t-1, j]:\n",
    "            path.append((token_path[j], t, emissions[t, token_path[j]].exp().item()))\n",
    "            j = j - 1\n",
    "        else:\n",
    "            path.append((token_path[j], t, emissions[t, token_path[j]].exp().item()))\n",
    "\n",
    "    path.reverse()\n",
    "    \n",
    "    # --- 4. Merge Segments and Format Output ---\n",
    "    # Filter out blank tokens and merge repeated characters\n",
    "    char_segments = []\n",
    "    for token, time_idx, score in path:\n",
    "        if token != blank_id:\n",
    "            char = processor.decode(token)\n",
    "            if not char_segments or char_segments[-1]['char'] != char:\n",
    "                char_segments.append({'char': char, 'start_frame': time_idx, 'end_frame': time_idx, 'scores': [score]})\n",
    "            else:\n",
    "                char_segments[-1]['end_frame'] = time_idx\n",
    "                char_segments[-1]['scores'].append(score)\n",
    "                \n",
    "    # Convert frame indices to seconds\n",
    "    ratio = waveform.shape[1] / emissions.shape[0] / 16000\n",
    "    word_segs = []\n",
    "    for seg in char_segments:\n",
    "        word_segs.append({\n",
    "            \"word\": seg['char'],\n",
    "            \"start\": round(seg['start_frame'] * ratio, 3),\n",
    "            \"end\": round((seg['end_frame'] + 1) * ratio, 3),\n",
    "            \"score\": round(np.mean(seg['scores']), 3)\n",
    "        })\n",
    "\n",
    "    return [{\n",
    "        \"start\": word_segs[0][\"start\"],\n",
    "        \"end\": word_segs[-1][\"end\"],\n",
    "        \"text\": transcription,\n",
    "        \"words\": word_segs\n",
    "    }]\n",
    "\n",
    "# --- Example Usage ---\n",
    "audio_file = \"/Users/anton/Downloads/audio_trim.wav\"\n",
    "transcript = \"ÊàëÂñúÊ¨¢Êú∫Âô®Â≠¶‰π†\"\n",
    "\n",
    "# Get the alignment\n",
    "result = align_audio_to_text(audio_file, transcript)\n",
    "\n",
    "# Print the final result\n",
    "import json\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83800cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
